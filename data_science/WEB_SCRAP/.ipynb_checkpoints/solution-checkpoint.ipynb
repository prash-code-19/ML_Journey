{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9d1342-8458-4075-8ef0-b87b9f230507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d6bfdc-a687-42a5-a9a8-c3b5641241e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded data from page 1\n",
      "downloaded data from page 2\n",
      "downloaded data from page 3\n",
      "downloaded data from page 4\n",
      "downloaded data from page 5\n",
      "downloaded data from page 6\n",
      "downloaded data from page 7\n",
      "downloaded data from page 8\n",
      "downloaded data from page 9\n",
      "downloaded data from page 10\n",
      "no more quotes..........\n"
     ]
    }
   ],
   "source": [
    "# fetching data\n",
    "\n",
    "page_count = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    URL = f\"https://quotes.toscrape.com/page/{page_count}/\"\n",
    "    res = requests.get(URL)\n",
    "\n",
    "    soup = BeautifulSoup(res.text,\"lxml\")\n",
    "    quotes = soup.select(\"div.quote\")\n",
    "\n",
    "    if not quotes:\n",
    "        print(\"no more quotes..........\")\n",
    "        break\n",
    "\n",
    "    with open(f\"scrapped_data/quotes{page_count}.html\", \"w\",encoding='utf-8') as f:\n",
    "        f.write(res.text)\n",
    "        print(f\"downloaded data from page {page_count}\")\n",
    "\n",
    "    page_count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ecef75-d8ea-449a-af62-1921f0967e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 992: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# extract useful info - page1\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mscrapped_data/quotes1.html\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     html_content = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m soup = BeautifulSoup(html_content,\u001b[33m\"\u001b[39m\u001b[33mlxml\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x9d in position 992: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# extract useful info - page1\n",
    "\n",
    "with open(\"scrapped_data/quotes1.html\",\"r\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948fb80-7eed-4b8d-a35a-45c55c9162b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
